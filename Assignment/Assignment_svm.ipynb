{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) is given by the decision function:\n",
    "\n",
    "𝑓\n",
    "(\n",
    "𝑥\n",
    ")\n",
    "=\n",
    "𝑤\n",
    "𝑇\n",
    "𝑥\n",
    "+\n",
    "𝑏\n",
    "f(x)=w \n",
    "T\n",
    " x+b\n",
    "\n",
    "where:\n",
    "\n",
    "𝑤\n",
    "w is the weight vector\n",
    "𝑥\n",
    "x is the input feature vector\n",
    "𝑏\n",
    "b is the bias term\n",
    "The decision boundary (hyperplane) is defined as the set of points where the decision function is zero:\n",
    "\n",
    "𝑤\n",
    "𝑇\n",
    "𝑥\n",
    "+\n",
    "𝑏\n",
    "=\n",
    "0\n",
    "w \n",
    "T\n",
    " x+b=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM?\n",
    "The objective function of a linear SVM is to find the hyperplane that maximizes the margin between the two classes while minimizing the classification error. The optimization problem can be written as:\n",
    "\n",
    "Minimize the following primal objective function:\n",
    "\n",
    "1\n",
    "2\n",
    "∥\n",
    "𝑤\n",
    "∥\n",
    "2\n",
    "+\n",
    "𝐶\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "𝜉\n",
    "𝑖\n",
    "2\n",
    "1\n",
    "​\n",
    " ∥w∥ \n",
    "2\n",
    " +C∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ξ \n",
    "i\n",
    "​\n",
    " \n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "𝑦\n",
    "𝑖\n",
    "(\n",
    "𝑤\n",
    "𝑇\n",
    "𝑥\n",
    "𝑖\n",
    "+\n",
    "𝑏\n",
    ")\n",
    "≥\n",
    "1\n",
    "−\n",
    "𝜉\n",
    "𝑖\n",
    ",\n",
    "𝜉\n",
    "𝑖\n",
    "≥\n",
    "0\n",
    ",\n",
    "𝑖\n",
    "=\n",
    "1\n",
    ",\n",
    "…\n",
    ",\n",
    "𝑛\n",
    "y \n",
    "i\n",
    "​\n",
    " (w \n",
    "T\n",
    " x \n",
    "i\n",
    "​\n",
    " +b)≥1−ξ \n",
    "i\n",
    "​\n",
    " ,ξ \n",
    "i\n",
    "​\n",
    " ≥0,i=1,…,n\n",
    "\n",
    "where:\n",
    "\n",
    "𝑤\n",
    "w is the weight vector\n",
    "𝑏\n",
    "b is the bias term\n",
    "𝐶\n",
    "C is the regularization parameter\n",
    "𝜉\n",
    "𝑖\n",
    "ξ \n",
    "i\n",
    "​\n",
    "  are the slack variables that allow for misclassifications\n",
    "𝑦\n",
    "𝑖\n",
    "y \n",
    "i\n",
    "​\n",
    "  are the class labels (\n",
    "±\n",
    "1\n",
    "±1)\n",
    "𝑥\n",
    "𝑖\n",
    "x \n",
    "i\n",
    "​\n",
    "  are the input feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM?\n",
    "The kernel trick allows SVMs to perform classification in higher-dimensional feature spaces without explicitly computing the coordinates in that space. Instead of mapping the data to a high-dimensional space, the kernel trick computes the inner products of the data in that space using a kernel function \n",
    "𝐾\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " ).\n",
    "\n",
    "Common kernel functions include:\n",
    "\n",
    "Linear kernel: \n",
    "𝐾\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "𝑥\n",
    "𝑖\n",
    "𝑇\n",
    "𝑥\n",
    "𝑗\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=x \n",
    "i\n",
    "T\n",
    "​\n",
    " x \n",
    "j\n",
    "​\n",
    " \n",
    "Polynomial kernel: \n",
    "𝐾\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    "𝑇\n",
    "𝑥\n",
    "𝑗\n",
    "+\n",
    "1\n",
    ")\n",
    "𝑑\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=(x \n",
    "i\n",
    "T\n",
    "​\n",
    " x \n",
    "j\n",
    "​\n",
    " +1) \n",
    "d\n",
    " \n",
    "Radial Basis Function (RBF) kernel: \n",
    "𝐾\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "exp\n",
    "⁡\n",
    "(\n",
    "−\n",
    "𝛾\n",
    "∥\n",
    "𝑥\n",
    "𝑖\n",
    "−\n",
    "𝑥\n",
    "𝑗\n",
    "∥\n",
    "2\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=exp(−γ∥x \n",
    "i\n",
    "​\n",
    " −x \n",
    "j\n",
    "​\n",
    " ∥ \n",
    "2\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM? Explain with an example.\n",
    "Support vectors are the data points that lie closest to the decision boundary (hyperplane) and are critical in defining the position and orientation of the hyperplane. These points are the most difficult to classify and directly influence the optimal hyperplane. In essence, support vectors are the points that determine the margin between the classes.\n",
    "\n",
    "Example:\n",
    "Consider a binary classification problem where we have two classes (red and blue). The SVM algorithm will find the hyperplane that maximizes the margin between the two classes. The support vectors are the data points that lie on the edge of this margin.\n",
    "\n",
    "\n",
    "In the above example, the support vectors are marked with circles. These points are crucial because if any of these points were removed, the position of the hyperplane would change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM.\n",
    "Hyperplane: The decision boundary that separates the classes. For a linear SVM, it is a straight line in 2D or a flat plane in higher dimensions.\n",
    "\n",
    "Marginal plane: The planes that are parallel to the hyperplane and pass through the support vectors. The distance between these planes is the margin.\n",
    "\n",
    "Hard margin: SVM with no tolerance for misclassification. It requires that all data points are correctly classified and lie outside the margin. This is feasible only if the data is linearly separable.\n",
    "\n",
    "Soft margin: SVM that allows some misclassification to achieve a better overall model, especially when the data is not linearly separable. It introduces slack variables \n",
    "𝜉\n",
    "𝑖\n",
    "ξ \n",
    "i\n",
    "​\n",
    "  to allow some points to lie inside the margin or be misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. SVM Implementation through Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features for visualization purposes\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "svm_clf = SVC(kernel='linear', C=1.0)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_decision_regions(X_train, y_train, clf=svm_clf, legend=2)\n",
    "plt.title('SVM Decision Boundary')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a linear SVM classifier from scratch using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features for visualization purposes\n",
    "y = iris.target\n",
    "\n",
    "# Convert labels to binary (-1 and 1)\n",
    "y = np.where(y == 0, -1, 1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w - np.dot(x_i, y[idx]))\n",
    "                    self.b -= self.learning_rate * y[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)\n",
    "\n",
    "# Train the SVM from scratch\n",
    "svm_scratch = LinearSVM()\n",
    "svm_scratch.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred_scratch = svm_scratch.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "print(f'Accuracy (Scratch): {accuracy_scratch:.2f}')\n",
    "\n",
    "# Comparison with scikit-learn implementation\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train a linear SVM classifier using scikit-learn\n",
    "svm_sklearn = SVC(kernel='linear', C=1.0)\n",
    "svm_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set using scikit-learn\n",
    "y_pred_sklearn = svm_sklearn.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model using scikit-learn\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f'Accuracy (scikit-learn): {accuracy_sklearn:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
