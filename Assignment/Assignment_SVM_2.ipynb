{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "In machine learning, particularly in Support Vector Machines (SVMs), kernel functions enable the transformation of data into higher-dimensional spaces to make it easier to find a linear separating hyperplane. Polynomial kernel functions are a type of kernel function that allow the creation of polynomial decision boundaries in the original feature space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Q2 How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of the SVC classifier with a polynomial kernel\n",
    "svm_clf = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of the SVC classifier with a polynomial kernel\n",
    "svm_clf = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "\n",
    "In Support Vector Regression (SVR), the \n",
    "𝜖\n",
    "ϵ parameter defines a margin of tolerance where no penalty is given to errors. Increasing the value of \n",
    "𝜖\n",
    "ϵ widens this margin, allowing more data points to fall within the margin without contributing to the loss function. As a result, the number of support vectors generally decreases because fewer data points fall outside the margin and become support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)?\n",
    "\n",
    "\n",
    "Kernel Function: The kernel function determines the shape of the decision boundary. Common kernels include linear, polynomial, and RBF (Radial Basis Function). The choice of kernel affects the model's ability to capture complex patterns. For example, the RBF kernel can handle non-linear relationships, while the linear kernel is suitable for linear relationships.\n",
    "\n",
    "C Parameter: The \n",
    "𝐶\n",
    "C parameter controls the trade-off between achieving a low error on the training data and minimizing the norm of the weights. A small \n",
    "𝐶\n",
    "C value creates a wider margin at the cost of more misclassifications, leading to a simpler model. A large \n",
    "𝐶\n",
    "C value aims for fewer misclassifications but can lead to overfitting.\n",
    "\n",
    "Epsilon Parameter: The \n",
    "𝜖\n",
    "ϵ parameter defines the margin of tolerance where no penalty is given to errors. A larger \n",
    "𝜖\n",
    "ϵ value allows for more tolerance, reducing the number of support vectors and potentially leading to a simpler model with higher bias. A smaller \n",
    "𝜖\n",
    "ϵ value results in more support vectors and can capture more details but may overfit the data.\n",
    "\n",
    "Gamma Parameter: The \n",
    "𝛾\n",
    "γ parameter, used in RBF and polynomial kernels, defines how far the influence of a single training example reaches. A small \n",
    "𝛾\n",
    "γ value means a large influence (smooth decision boundary), while a large \n",
    "𝛾\n",
    "γ value means a small influence (complex decision boundary). Tuning \n",
    "𝛾\n",
    "γ is essential for balancing model complexity and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "# Print classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found by GridSearchCV\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred_best = best_svc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the tuned classifier\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f'Accuracy of Tuned Classifier: {accuracy_best:.2f}')\n",
    "\n",
    "\n",
    "# Save the trained classifier\n",
    "joblib.dump(best_svc, 'tuned_svc_model.pkl')\n",
    "\n",
    "# Load the saved model (example of how to load the saved model)\n",
    "loaded_model = joblib.load('tuned_svc_model.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
