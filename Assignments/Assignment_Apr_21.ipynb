{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmPbyuBcW3R+H8sbDDKpnh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
        "\n",
        "The difference between the Euclidean and Manhattan distance metrics in KNN is that the Euclidean distance metric takes into account the magnitude of the difference between two points, while the Manhattan distance metric only takes into account the absolute difference.\n",
        "\n",
        "Euclidean distance is more commonly used in KNN.\n",
        "\n",
        "Manhattan distance metric may be a better choice for datasets with outliers."
      ],
      "metadata": {
        "id": "tontb4QxyHFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
        "\n",
        "Methods such as Cross validation and grid search CV can be used to determine the optimal k values."
      ],
      "metadata": {
        "id": "l5Tu7huQyHC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
        "\n",
        " Different distance metrics may give different results for the same dataset. This is because they measure distance in different ways. For example, Euclidean distance is sensitive to magnitudes and can be greatly affected by features that vary over large ranges, while Manhattan distance is not.\n",
        "\n",
        " Euclidean Distance: This is the most commonly used distance metric. It's suitable for continuous variables and works best if the variables are on the same scale.\n",
        "Manhattan Distance: This can be useful when you have binary or categorical variables."
      ],
      "metadata": {
        "id": "GsEYfGlEyHAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
        "\n",
        "- number of neighbors (k) and the distance metric.\n",
        "\n",
        "    * Number of neighbors (k): A higher value of k will generally result in better performance on noisy data, but it can also lead to overfitting. A lower value of k will generally be more robust to overfitting, but it may not perform as well on noisy data.\n",
        "\n",
        "    * Distance metric: The choice of distance metric can have a significant impact on the performance of the KNN model. It is important to choose a distance metric that is appropriate for the nature of the data.\n",
        "\n",
        "- To tune the hyperparameters of a KNN model, one can use a grid search or a random search."
      ],
      "metadata": {
        "id": "gVK15FT6yG9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
        "\n",
        "A larger training set will generally result in a more accurate model. However, using a larger training set can also lead to overfitting. Overfitting occurs when the model learns the training data too well and is unable to generalize to new data.\n",
        "\n",
        "* Use a small number of features for classification tasks, and a large number of features for regression tasks.\n",
        "* Normalize the data before using KNN.\n",
        "* Use a distance metric that is robust to missing values, if applicable.\n",
        "* Use a regularization technique, such as L1 or L2 regularization, to prevent overfitting."
      ],
      "metadata": {
        "id": "0RP92uf8yG6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n",
        "\n",
        "drawbacks of KNN:\n",
        "\n",
        "- Computational complexity\n",
        "- Sensitivity to outliers\n",
        "- Overfitting\n",
        "- Curse of dimensionality\n",
        "\n",
        "To overcome the drawbacks:\n",
        "\n",
        "- Use a robust distance metric\n",
        "- Use a regularization technique\n",
        "- Use a cross-validation procedure\n",
        "- Use a dimensionality reduction technique"
      ],
      "metadata": {
        "id": "5IRRZUpnyG4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UHcdM02syG1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "06GBKfu1yGyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rX66sDjwyGv8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajxJbox8yC16"
      },
      "outputs": [],
      "source": []
    }
  ]
}