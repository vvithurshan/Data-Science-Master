{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752680e7-d285-423b-93ef-ea2fe672f310",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ff9ea-94fa-4d5e-9594-4541ebe995c8",
   "metadata": {},
   "source": [
    "R-squared measures how well the linear regression model fits the data. It ranges from 0 to 1, with a higher value indicating a better fit. It can be calculated using the following formaul\n",
    "\n",
    "- R-squared = 1 - (sum of squared residuals / total sum of squares)\n",
    "\n",
    "    - sum of squared residuals is the sum of the squared differences between the predicted values and the actual values.         \n",
    "    - total sum of squares is the sum of the squared differences between the actual values and the mean of the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4072b6b-ccfc-489c-8e67-388eac1c3711",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8af292-9ece-40cc-be3a-f54561490043",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of the regular R-squared that takes into account the number of independent variables in a linear regression model. While R-squared represents the proportion of variance in the dependent variable that is explained by the independent variables, adjusted R-squared adjusts this measure for the number of independent variables in the model.\n",
    "\n",
    "Adjusted R-squared is calculated using the following formula:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "R-squared is the regular coefficient of determination\n",
    "n is the sample size\n",
    "k is the number of independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72255a-1b1c-4b33-92de-706429cb2d91",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0261819-61d8-42c9-b956-1eb2ae8ad181",
   "metadata": {},
   "source": [
    "It is more appropriate to use when the model has a large number of independent variables. This is because as the number of independent variables increases, the R-squared will also increase, even if the model does not actually fit the data better. Adjusted R-squared helps to prevent this overfitting by adjusting the R-squared for the number of independent variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6eee9e-1b2e-407f-b615-633523192a87",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dce5a4-406e-4462-a1a7-a181df18d07b",
   "metadata": {},
   "source": [
    "RMSE, MSE, and MAE are metrics commonly used to evaluate the performance of regression models. \n",
    "\n",
    "    MSE is calculated as the average of the squared differences between the predicted and actual values. A lower MSE indicates a better fit. \n",
    "\n",
    "    RMSE is the square root of the MSE, which gives the error in the same units as the target variable, and is commonly used to compare the performance of different models. \n",
    "\n",
    "    MAE is calculated as the average of the absolute differences between the predicted and actual values. It represents the average magnitude of the errors in the predictions, and is less sensitive to outliers compared to MSE and RMSE. \n",
    "\n",
    "In general, RMSE, MSE, and MAE indicate the error of the model. Lower value indicates better performance of the model in predicting the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e561a-b134-4ddf-81d2-77f116e3a54e",
   "metadata": {},
   "source": [
    "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a4bd8-8829-48d3-909a-f56f4c8c179b",
   "metadata": {},
   "source": [
    "RMSE \n",
    "advantages\n",
    "    - it has the same unit of measurements as the target variable, easier to interpret\n",
    "    - it is differentiable, gradient descent algorithm can be used find the global minimum.\n",
    "disadvantages\n",
    "    - it is not robust to outliers\n",
    "\n",
    "MSE \n",
    "advantages \n",
    "    - it is differentiable, gradient descent algorithm can be used find the global minimum.\n",
    "    - it has one local minima and one global minima\n",
    "disadvantages\n",
    "    - it is not robust to outliers\n",
    "    - it has a different unit of measurements to the target variable. \n",
    "    \n",
    "MAE\n",
    "advantages\n",
    "    - it is roubut to outliers than MSE\n",
    "    - it has the same unit of measurements as the target variable\n",
    "disadvantages\n",
    "    - It is not differentiable at 0, making the optimization complex.\n",
    "    - It may not be appropriate for skewed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61bb437-8fe4-44f3-8406-3eab2eb93ff1",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c90c1-9554-4322-9100-2a0ea92bc4b6",
   "metadata": {},
   "source": [
    "Lasso regularization is a type of shrinkage regularization that shrinks the coefficients of the independent variables towards zero. This is done by adding a penalty term to the loss function that is proportional to the sum of the absolute values of the coefficients. As a result, some of the coefficients may be shrunk to zero, which effectively removes those variables from the model.\n",
    "\n",
    "Ridge regularization, L2 regularization, is another type of shrinkage regularization. However, it differs from Lasso regularization in that it shrinks the coefficients of the independent variables towards zero in a quadratic fashion. This means that the coefficients are less likely to be shrunk to zero than with Lasso regularization.\n",
    "\n",
    "Lasso regularization is more appropriate to do feature selection. This is because it can automatically identify the most important features and remove the less important features from the model. Ridge regularization is more appropriate to use to prevent overfitting, but it does not to do feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c635e5bb-a7dc-4ce4-8863-564535a5fa31",
   "metadata": {},
   "source": [
    "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbcd0f3-14d9-4e31-b00b-8bf8b5ebbe75",
   "metadata": {},
   "source": [
    "Regularized linear models help to prevent overfitting in machine learning by penalizing the model for having too many parameters. This encourages the model to learn a simpler model that is less likely to overfit the training data.\n",
    "\n",
    "For example, a linear regression model that is trained on a dataset of house prices. The model has 100 features, each of which is a measure of the house, such as the number of bedrooms, the square footage, and the location. Without regularization, the model will likely learn a complex model that fits the training data very well. However, this model is also likely to overfit the training data, meaning that it will not generalize well to new data. With regularization, the model will be penalized for having too many parameters. This will encourage the model to learn a simpler model that is less likely to overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d60b9e-a932-48d3-84bc-cfa91a9649e4",
   "metadata": {},
   "source": [
    "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the bestchoice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a0f13-c778-4fa2-b8f3-55be5fa4537d",
   "metadata": {},
   "source": [
    "- **They can be computationally expensive to train.** This is because the regularization term adds an additional term to the loss function, which can make the optimization problem more difficult to solve.\n",
    "- **They can be sensitive to the choice of regularization strength.** If the regularization strength is too high, the model will be too simple and will not be able to learn the full complexity of the data. If the regularization strength is too low, the model will be too complex and will overfit the training data.\n",
    "- **They can be biased.** This is because the regularization term penalizes the model for having too many parameters, which can prevent it from learning the full complexity of the data.\n",
    "- **They can be difficult to interpret.** This is because the regularization term can shrink the coefficients of the independent variables towards zero, which can make it difficult to understand the relationships between the independent variables and the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a69e43-1118-43ec-9948-311a4fc109d7",
   "metadata": {},
   "source": [
    "## Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fecfe8-15eb-4c7b-a404-a0512f070193",
   "metadata": {},
   "source": [
    "I would choose Model B as the better performer. This is because the MAE is less sensitive to outliers than the RMSE. They can skew the results of the RMSE, making it seem like the model is not as accurate as it actually is. The MAE is not affected by outliers as much, so it is a more reliable measure of the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f428e131-0f4f-4390-ae55-f690c66574ee",
   "metadata": {},
   "source": [
    "## Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e314e7-e8cb-4e5c-81f5-dcff77790d44",
   "metadata": {},
   "source": [
    "Ridge regularization penalizes the sum of the squares of the coefficients, while Lasso regularization penalizes the sum of the absolute values of the coefficients. This means that Ridge regularization is more likely to shrink the coefficients of all features towards zero, while Lasso regularization is more likely to shrink some coefficients to zero and leave others unchanged. In this case, the regularization parameter for Lasso regularization is 0.5, which is a relatively large value. This means that it is more likely that Lasso regularization will shrink some coefficients to zero.\n",
    "\n",
    "Therefore, I would choose Model B as the better performer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bac1a-43b5-4d78-8782-7c9e36c19daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
